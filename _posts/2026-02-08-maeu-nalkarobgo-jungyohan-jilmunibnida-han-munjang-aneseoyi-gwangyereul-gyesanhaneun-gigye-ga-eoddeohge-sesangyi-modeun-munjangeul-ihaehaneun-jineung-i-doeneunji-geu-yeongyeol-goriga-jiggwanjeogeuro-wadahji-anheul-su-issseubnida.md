---
layout: post
title: "매우 날카롭고 중요한 질문입니다. \"한 문장 안에서의 관계를 계산하는 기계\"가 어떻게 \"세상의 모든 문장을 이해하는 지능\"이 되는지, 그 연결 고리가 직관적으로 와닿지 않을 수 있습니다."
date: 2026-02-08 22:33:50 +0900
categories: [DiscordLog]
tags: [til, dev]
---

매우 날카롭고 중요한 질문입니다. "한 문장 안에서의 관계를 계산하는 기계"가 어떻게 "세상의 모든 문장을 이해하는 지능"이 되는지, 그 연결 고리가 직관적으로 와닿지 않을 수 있습니다.
핵심은 **"문장을 외우는 것이 아니라, 관계를 처리하는 '공식(함수)'을 학습하기 때문"**입니다.
개발자 용어로 설명하자면, 데이터(Sentence)는 바뀌어도 로직(Weights)은 공유된다는 점이 비결입니다. 3가지 핵심 원리로 설명해 드리겠습니다.
1. 가중치 공유 (Parameter Sharing): 만능 도구 세트
앞서 설명한 W^Q, W^K, W^V라는 행렬들은 문장마다 새로 만들어지는 것이 아닙니다. 하나의 거대한 모델이 수조 개의 문장을 읽으면서 최적화한, 딱 하나의 세트입니다.
• 학습 과정:
• 모델은 "나는 사과를 먹는다"를 봅니다. '먹는다'와 '사과'의 관계를 높게 평가하도록 W 행렬을 미세 조정합니다.
• 다음엔 "그는 물을 마신다"를 봅니다. '마신다'와 '물'의 관계를 높게 평가하도록 W를 또 수정합니다.
• 이 과정을 수천억 번 반복합니다.
• 결과:
• W 행렬은 이제 특정 단어가 아니라, **"동사(서술어)는 목적어와 강한 연관이 있다"**는 **보편적인 언어의 규칙(패턴)**을 담게 됩니다.
• 그래서 처음 보는 문장인 "철수는 축구를 한다"가 들어와도, 이미 학습된 W 행렬이 '한다'와 '축구'를 찰떡같이 연결해냅니다.
즉, 개별 문장을 암기하는 게 아니라, 문장 성분끼리 상호작용하는 '물리 법칙'을 학습한 것입니다.
2. 임베딩 공간의 일반화 (Semantic Vector Space)
단어들이 숫자로 변환된 임베딩(Embedding) 덕분에 가능합니다.
• 컴퓨터 공간 상에서 '사과'와 '배', '바나나'는 아주 가까운 위치에 모여 있습니다. (좌표가 비슷함)
• 모델이 "사과를 깎다"라는 문장을 배워서 '사과'와 '깎다'의 관계(Attention)를 학습했다고 칩시다.
• 나중에 "배를 깎다"라는 안 배운 문장이 들어와도, '배'의 좌표가 '사과'와 비슷하기 때문에 어텐션 연산 결과도 비슷하게 나옵니다.
이 그림처럼 비슷한 단어들이 뭉쳐 있기 때문에, 한 문장에서 배운 관계가 유사한 다른 단어가 쓰인 수만 가지 문장으로 **자동 확장(Generalization)**됩니다.
3. 언어의 조합성 (Compositionality)
언어는 레고 블록과 같습니다. 우리는 "빨간 코끼리가 날아다닌다"라는 문장을 평생 한 번도 본 적 없어도 바로 이해할 수 있습니다. 왜냐하면 '빨간', '코끼리', '날아다닌다'라는 블록의 기능을 알기 때문입니다.
• 트랜스포머는 수많은 데이터를 통해 블록 조립 설명서를 익힌 상태입니다.
• "형용사는 명사를 꾸민다."
• "주어가 복수면 동사도 복수형이어야 한다."
• "접속사 '하지만' 뒤에는 앞 내용과 반대되는 내용이 온다."
• 이 '조립 설명서(어텐션 패턴)'를 가지고 있기 때문에, 단어(블록)만 갈아 끼워진 새로운 문장이 와도 그 구조를 꿰뚫어 볼 수 있는 것입니다.
요약
질문하신 "어떻게 가능한가?"에 대한 답은 다음과 같습니다.
1. 패턴의 추출: 수많은 문장을 보면서 단어 그 자체가 아니라 **단어와 단어 사이의 추상적인 관계 법칙(문법, 문맥, 논리)**을 W 행렬에 압축해 넣었기 때문입니다.
2. 유사성 활용: 비슷한 단어는 비슷한 방식으로 작동한다는 벡터 공간의 특성을 이용해, 안 배운 문장도 배운 문장처럼 처리하기 때문입니다.
결국 트랜스포머는 세상의 모든 문장을 '기억'하고 있는 게 아니라, 어떤 문장이 들어와도 해석할 수 있는 '해석 능력(함수)'을 가지고 있는 것입니다.
